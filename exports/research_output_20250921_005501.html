
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Report</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; }
        h1 { color: #333; border-bottom: 2px solid #333; }
        h2 { color: #666; margin-top: 30px; }
        h3 { color: #888; }
        .confidence { font-weight: bold; }
        .confidence.high { color: green; }
        .confidence.medium { color: orange; }
        .confidence.low { color: red; }
        .evidence { background-color: #f5f5f5; padding: 10px; margin: 10px 0; border-left: 4px solid #007acc; }
    </style>
</head>
<body>
    <h1>Research Report</h1>
    <p><strong>Query:</strong> what is sgld?</p>
    <p><strong>Generated:</strong> 2025-09-21 00:55:01</p>
    
    <h2>Executive Summary</h2>
    <div>ğŸ” DEEP RESEARCH RESULTS<br>============================================================<br>Query: what is sgld?<br>Analysis Date: 2025-09-21 00:55:01<br><br>ğŸ“‹ EXECUTIVE SUMMARY<br>------------------------------<br>The research reveals several findings with moderate confidence:<br>DETAILED ANALYSIS<br>==================================================<br><br>1. what is sgld?<br>----------------------------------------<br>Confidence: ğŸŸ¡ 0.66<br><br>Answer:<br>Exact Bayesian Learning is possible on a limited set of models as we need to<br>know the prior  Â  Â  Â  Â  Â  Â  Â  Â of the model parameters which is not feasible.<br>When x and t are independent, it means that the likelihood function does not<br>depend on the covariates.<br><br>Sources (5):<br>  [1] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.15<br>  [2] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.15<br>  [3] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15<br>  [4] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15<br>  [5] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15<br><br>CONFIDENCE ASSESSMENT<br>==================================================<br>Overall Confidence: Moderate (0.66)<br>Assessment: Good evidence with some limitations<br><br>Evidence Quality:<br>  â€¢ Total evidence pieces: 5<br>  â€¢ Source diversity: 2 unique sources<br>  â€¢ Subtasks completed: 1<br><br>SOURCE VERIFICATION<br>==================================================<br>Sources analyzed: 2<br><br>============================================================<br>Research completed by Deep Researcher Agent<br>All sources are from local document collection</div>
    
    <h2>Detailed Analysis</h2>

    <h3>1. what is sgld?</h3>
    <p><strong>Answer:</strong> Exact Bayesian Learning is possible on a limited set of models as we need to know the prior  Â  Â  Â  Â  Â  Â  Â  Â of the model parameters which is not feasible. When x and t are independent, it means that the likelihood function does not depend on the covariates.</p>
    <p><strong>Confidence:</strong> <span class="confidence medium">0.66</span></p>
<h4>Evidence:</h4>
    <div class="evidence">
        <strong>1. bayesian presentation iisc.pdf</strong> (relevance: 0.150)<br>
        ion that the posterior is the Gaussian Distribution 
ğ‘(Î¸âˆ£ğ·)=ğ’©(Î¸âˆ£âˆ£Î½,Ïƒ2ğ¼) for some mean ğœˆ and variance ğœ2. Logistic regression sets the likelihood 
as p(t=1âˆ£x,Î¸)=Ïƒ(Î¸âŠ¤u(x)) for a feature of vectors u(x). For this model , what shall be the 
ensemble predictive distribution  
ğ‘(ğ‘¡âˆ£ğ‘¥,ğ·)=ğ¸Î¸âˆ¼â„•(Î¸âˆ£âˆ£âˆ£Î½,ÏƒğŸšğ¼)[Ïƒ(Î¸...
    </div>

    <div class="evidence">
        <strong>2. bayesian presentation iisc.pdf</strong> (relevance: 0.150)<br>
        gets t 
directly, rather than modelling the conditional distribution given inputs x. This setup is most common 
in density estimation problems such as:  
- Estimating a distribution from samples (e.g. Estimating a gaussian distribution from data)  
- Learning a generative model for the data  
Basica...
    </div>

    <div class="evidence">
        <strong>3. NPTEL Summer Internship Report.pdf</strong> (relevance: 0.150)<br>
        ğ¹(ğ‘)=âˆ’ğ¸ğ‘[logğ‘(ğ·|ğœƒ)]+ğ¾ğ¿(ğ‘(ğœƒ)âˆ¥ğ‘(ğœƒ)) 
 
Minimize $F(q)$ â‡” Maximize ELBO(q) : 
 
ğ¹(ğ‘(ğœƒ|ğ·))=âˆ’ğ¸ğ‘[logğ‘(ğ·|ğœƒ)]+ğ¾ğ¿(ğ‘(ğœƒ|ğ·)âˆ¥ğ‘(ğœƒ)) 
âˆ’ğ¸ğ‘[logğ‘(ğ·|ğœƒ)]=âˆ‘ğ¸ğ‘[logğ‘(ğ‘¦ğ‘›|ğ‘¥ğ‘›,ğœƒ)]ğ‘
ğ‘›=1=ğ‘â‹…ğ¸[ğ¿ğ·(ğœƒ)] 
 
Average â€“ve log likelihood. Multiply by N or not to enforce. We measure per sample 
performance to make it dataset irrelevant. K...
    </div>

    <div class="evidence">
        <strong>4. NPTEL Summer Internship Report.pdf</strong> (relevance: 0.150)<br>
        Exact Bayesian Learning is possible on a limited set of models as we need to know the prior 
of the model parameters which is not feasible. Now we will talk about the scenarios where it 
is feasi ble. 3.1 Conjugate Priors  
 
3.2 When x and t are Independent â€“ SPECIAL CASE  
When x and t are indepen...
    </div>

    <div class="evidence">
        <strong>5. NPTEL Summer Internship Report.pdf</strong> (relevance: 0.150)<br>
        n distribution from 
data)  
- Learning a generative model for the data  
Basically, what happens since x doesnâ€™t influence the likelihood, the learning problem becomes 
independent of the covariates. We only update the beliefs about ğœƒ based on observed data t, 
without needing to condition on x. In...
    </div>

</body>
</html>
