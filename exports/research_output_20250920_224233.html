
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Report</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; }
        h1 { color: #333; border-bottom: 2px solid #333; }
        h2 { color: #666; margin-top: 30px; }
        h3 { color: #888; }
        .confidence { font-weight: bold; }
        .confidence.high { color: green; }
        .confidence.medium { color: orange; }
        .confidence.low { color: red; }
        .evidence { background-color: #f5f5f5; padding: 10px; margin: 10px 0; border-left: 4px solid #007acc; }
    </style>
</head>
<body>
    <h1>Research Report</h1>
    <p><strong>Query:</strong> what is sgld
</p>
    <p><strong>Generated:</strong> 2025-09-20 22:42:34</p>
    
    <h2>Executive Summary</h2>
    <div>üîç DEEP RESEARCH RESULTS<br>============================================================<br>Query: what is sgld<br><br>Analysis Date: 2025-09-20 22:42:33<br><br>üìã EXECUTIVE SUMMARY<br>------------------------------<br>Based on the analysis of available sources, the research provides strong evidence for the following findings:<br>DETAILED ANALYSIS<br>==================================================<br><br>1. what is sgld<br>----------------------------------------<br>Confidence: üü¢ 0.72<br><br>Answer:<br>Stochastic Gradient Langevin Dynamics (SGLD) is a modern MCMC method that scales<br>Bayesian sampling to large datasets and models. SGLD uses min i-batches to<br>compute a noisy gradient of the log -likelihood, adds Gaussian noise, and slowly<br>decays the step size.<br><br>Sources (5):<br>  [1] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.30<br>  [2] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.30<br>  [3] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.30<br>  [4] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.30<br>  [5] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.30<br><br>CONFIDENCE ASSESSMENT<br>==================================================<br>Overall Confidence: Moderate (0.72)<br>Assessment: Good evidence with some limitations<br><br>Evidence Quality:<br>  ‚Ä¢ Total evidence pieces: 5<br>  ‚Ä¢ Source diversity: 2 unique sources<br>  ‚Ä¢ Subtasks completed: 1<br><br>SOURCE VERIFICATION<br>==================================================<br>Sources analyzed: 2<br><br>============================================================<br>Research completed by Deep Researcher Agent<br>All sources are from local document collection</div>
    
    <h2>Detailed Analysis</h2>

    <h3>1. what is sgld</h3>
    <p><strong>Answer:</strong> Stochastic Gradient Langevin Dynamics (SGLD) is a modern MCMC method that scales Bayesian sampling to large datasets and models. SGLD uses min i-batches to compute a noisy gradient of the log -likelihood, adds Gaussian noise, and slowly decays the step size.</p>
    <p><strong>Confidence:</strong> <span class="confidence high">0.72</span></p>
<h4>Evidence:</h4>
    <div class="evidence">
        <strong>1. bayesian presentation iisc.pdf</strong> (relevance: 0.300)<br>
        imension with local minima  

Stochastic Gradient Markov Chain Monte Carlo  
MCMC -MH requires full access of the dataset at each iteration to compute posterior which is very slow 
for large datasets. Stochastic Gradient MCMC uses  
- Mini Batches of data instead of full dataset  
- Combining ideas ...
    </div>

    <div class="evidence">
        <strong>2. bayesian presentation iisc.pdf</strong> (relevance: 0.300)<br>
        ike stochastic gradient descent) but injects carefully scaled noise so that in the limit it samples from 
the posterior. In effect, one treats the optimization trajectory itself as a sampling process. Practically, 
SGLD uses min i-batches to compute a noisy gradient of the log -likelihood, adds Gaus...
    </div>

    <div class="evidence">
        <strong>3. bayesian presentation iisc.pdf</strong> (relevance: 0.300)<br>
        y essentially running a noisy 
SGD. SGLD has been applied, for example, to Bayesian deep learning, letting neural networks capture 
uncertainty by treating the training updates as a sampling process . GD -> moves deterministically in direction of gradient + LD -> adds gaussian noise to simulate diff...
    </div>

    <div class="evidence">
        <strong>4. NPTEL Summer Internship Report.pdf</strong> (relevance: 0.300)<br>
        n with local minima  
 
5.3 Stochastic Gradient Markov Chain Monte Carlo  
 
MCMC -MH requires full access of the dataset at each iteration to compute posterior which is 
very slow for large datasets. Stochastic Gradient MCMC uses  
- Mini Batches of data instead of full dataset  
- Combining ideas ...
    </div>

    <div class="evidence">
        <strong>5. NPTEL Summer Internship Report.pdf</strong> (relevance: 0.300)<br>
        ke stochastic gradient descent) but injects carefully 
scaled noise so that in the limit it samples from the posterior. In effect, one treats the 
optimization trajectory itself as a sampling process. Practically, SGLD uses min i-batches to 
compute a noisy gradient of the log -likelihood, adds Gaus...
    </div>

</body>
</html>
