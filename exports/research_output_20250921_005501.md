# Research Report

**Query:** what is sgld?

**Generated:** 2025-09-21 00:55:01

## Executive Summary

🔍 DEEP RESEARCH RESULTS
============================================================
Query: what is sgld?
Analysis Date: 2025-09-21 00:55:01

📋 EXECUTIVE SUMMARY
------------------------------
The research reveals several findings with moderate confidence:
DETAILED ANALYSIS
==================================================

1. what is sgld?
----------------------------------------
Confidence: 🟡 0.66

Answer:
Exact Bayesian Learning is possible on a limited set of models as we need to
know the prior                 of the model parameters which is not feasible.
When x and t are independent, it means that the likelihood function does not
depend on the covariates.

Sources (5):
  [1] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [2] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [3] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [4] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [5] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15

CONFIDENCE ASSESSMENT
==================================================
Overall Confidence: Moderate (0.66)
Assessment: Good evidence with some limitations

Evidence Quality:
  • Total evidence pieces: 5
  • Source diversity: 2 unique sources
  • Subtasks completed: 1

SOURCE VERIFICATION
==================================================
Sources analyzed: 2

============================================================
Research completed by Deep Researcher Agent
All sources are from local document collection

## Detailed Analysis

### 1. what is sgld?

**Answer:** Exact Bayesian Learning is possible on a limited set of models as we need to know the prior                 of the model parameters which is not feasible. When x and t are independent, it means that the likelihood function does not depend on the covariates.

**Confidence:** 0.66

**Evidence:**

1. **bayesian presentation iisc.pdf** (relevance: 0.150)
   ion that the posterior is the Gaussian Distribution 
𝑝(θ∣𝐷)=𝒩(θ∣∣ν,σ2𝐼) for some mean 𝜈 and variance 𝜎2. Logistic regression sets the likelihood 
as p(t=1∣x,θ)=σ(θ⊤u(x)) for a feature of vectors u(x). For this model , what shall be the 
ensemble predictive distribution  
𝑝(𝑡∣𝑥,𝐷)=𝐸θ∼ℕ(θ∣∣∣ν,σ𝟚𝐼)[σ(θ⊤𝑢(𝑥))] 
𝑝(𝑡∣𝑥,𝐷)≈σ
( ν⊤𝑢(𝑥)
√1+πσ2
8)  
A larger variance σ2 of the posterior decreases the confidence of the prediction by reducing t he 
absolute value of the logit (i.e. of the argument of the sig...

2. **bayesian presentation iisc.pdf** (relevance: 0.150)
   gets t 
directly, rather than modelling the conditional distribution given inputs x. This setup is most common 
in density estimation problems such as:  
- Estimating a distribution from samples (e.g. Estimating a gaussian distribution from data)  
- Learning a generative model for the data  
Basically, what happens since x doesn’t influence the likelihood, the learning problem becomes 
independent of the covariates. We only update the beliefs about 𝜃 based on observed data t, without 
needing t...

3. **NPTEL Summer Internship Report.pdf** (relevance: 0.150)
   𝐹(𝑞)=−𝐸𝑞[log𝑝(𝐷|𝜃)]+𝐾𝐿(𝑞(𝜃)∥𝑝(𝜃)) 
 
Minimize $F(q)$ ⇔ Maximize ELBO(q) : 
 
𝐹(𝑞(𝜃|𝐷))=−𝐸𝑞[log𝑝(𝐷|𝜃)]+𝐾𝐿(𝑞(𝜃|𝐷)∥𝑝(𝜃)) 
−𝐸𝑞[log𝑝(𝐷|𝜃)]=∑𝐸𝑞[log𝑝(𝑦𝑛|𝑥𝑛,𝜃)]𝑁
𝑛=1=𝑁⋅𝐸[𝐿𝐷(𝜃)] 
 
Average –ve log likelihood. Multiply by N or not to enforce. We measure per sample 
performance to make it dataset irrelevant. KL divergence naturally appears in derivation when we try to match 𝑞(𝜃|𝐷) to true posterior. It tells how one diverges from other – exactly what we want. Also,  KL divergence is directly 
tied to ELBO....

4. **NPTEL Summer Internship Report.pdf** (relevance: 0.150)
   Exact Bayesian Learning is possible on a limited set of models as we need to know the prior 
of the model parameters which is not feasible. Now we will talk about the scenarios where it 
is feasi ble. 3.1 Conjugate Priors  
 
3.2 When x and t are Independent – SPECIAL CASE  
When x and t are independent, it means that the likelihood function does not depend on the 
input covariates x, and we can write:  
 
𝑝(𝑡|𝑥,𝜃)=𝑝(𝑡|𝜃) 

16 
  
This simplification leads to a scenario where you’re essentially ...

5. **NPTEL Summer Internship Report.pdf** (relevance: 0.150)
   n distribution from 
data)  
- Learning a generative model for the data  
Basically, what happens since x doesn’t influence the likelihood, the learning problem becomes 
independent of the covariates. We only update the beliefs about 𝜃 based on observed data t, 
without needing to condition on x. In such a scenario the posterior 𝑝(θ∣𝐷) is computed 
simply by updating the natural parameters of the prior with sufficient statistics from the data. 𝑡𝑛∼𝒩(μ,σ2) 
Here μ is unknown, σ2 is known and there...

---

