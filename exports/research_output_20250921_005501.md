# Research Report

**Query:** what is sgld?

**Generated:** 2025-09-21 00:55:01

## Executive Summary

ğŸ” DEEP RESEARCH RESULTS
============================================================
Query: what is sgld?
Analysis Date: 2025-09-21 00:55:01

ğŸ“‹ EXECUTIVE SUMMARY
------------------------------
The research reveals several findings with moderate confidence:
DETAILED ANALYSIS
==================================================

1. what is sgld?
----------------------------------------
Confidence: ğŸŸ¡ 0.66

Answer:
Exact Bayesian Learning is possible on a limited set of models as we need to
know the prior  Â  Â  Â  Â  Â  Â  Â  Â of the model parameters which is not feasible.
When x and t are independent, it means that the likelihood function does not
depend on the covariates.

Sources (5):
  [1] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [2] bayesian presentation iisc.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [3] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [4] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15
  [5] NPTEL Summer Internship Report.pdf (n.d.). Retrieved from local database. Relevance score: 0.15

CONFIDENCE ASSESSMENT
==================================================
Overall Confidence: Moderate (0.66)
Assessment: Good evidence with some limitations

Evidence Quality:
  â€¢ Total evidence pieces: 5
  â€¢ Source diversity: 2 unique sources
  â€¢ Subtasks completed: 1

SOURCE VERIFICATION
==================================================
Sources analyzed: 2

============================================================
Research completed by Deep Researcher Agent
All sources are from local document collection

## Detailed Analysis

### 1. what is sgld?

**Answer:** Exact Bayesian Learning is possible on a limited set of models as we need to know the prior  Â  Â  Â  Â  Â  Â  Â  Â of the model parameters which is not feasible. When x and t are independent, it means that the likelihood function does not depend on the covariates.

**Confidence:** 0.66

**Evidence:**

1. **bayesian presentation iisc.pdf** (relevance: 0.150)
   ion that the posterior is the Gaussian Distribution 
ğ‘(Î¸âˆ£ğ·)=ğ’©(Î¸âˆ£âˆ£Î½,Ïƒ2ğ¼) for some mean ğœˆ and variance ğœ2. Logistic regression sets the likelihood 
as p(t=1âˆ£x,Î¸)=Ïƒ(Î¸âŠ¤u(x)) for a feature of vectors u(x). For this model , what shall be the 
ensemble predictive distribution  
ğ‘(ğ‘¡âˆ£ğ‘¥,ğ·)=ğ¸Î¸âˆ¼â„•(Î¸âˆ£âˆ£âˆ£Î½,ÏƒğŸšğ¼)[Ïƒ(Î¸âŠ¤ğ‘¢(ğ‘¥))] 
ğ‘(ğ‘¡âˆ£ğ‘¥,ğ·)â‰ˆÏƒ
( Î½âŠ¤ğ‘¢(ğ‘¥)
âˆš1+Ï€Ïƒ2
8)  
A larger variance Ïƒ2 of the posterior decreases the confidence of the prediction by reducing t he 
absolute value of the logit (i.e. of the argument of the sig...

2. **bayesian presentation iisc.pdf** (relevance: 0.150)
   gets t 
directly, rather than modelling the conditional distribution given inputs x. This setup is most common 
in density estimation problems such as:  
- Estimating a distribution from samples (e.g. Estimating a gaussian distribution from data)  
- Learning a generative model for the data  
Basically, what happens since x doesnâ€™t influence the likelihood, the learning problem becomes 
independent of the covariates. We only update the beliefs about ğœƒ based on observed data t, without 
needing t...

3. **NPTEL Summer Internship Report.pdf** (relevance: 0.150)
   ğ¹(ğ‘)=âˆ’ğ¸ğ‘[logğ‘(ğ·|ğœƒ)]+ğ¾ğ¿(ğ‘(ğœƒ)âˆ¥ğ‘(ğœƒ)) 
 
Minimize $F(q)$ â‡” Maximize ELBO(q) : 
 
ğ¹(ğ‘(ğœƒ|ğ·))=âˆ’ğ¸ğ‘[logğ‘(ğ·|ğœƒ)]+ğ¾ğ¿(ğ‘(ğœƒ|ğ·)âˆ¥ğ‘(ğœƒ)) 
âˆ’ğ¸ğ‘[logğ‘(ğ·|ğœƒ)]=âˆ‘ğ¸ğ‘[logğ‘(ğ‘¦ğ‘›|ğ‘¥ğ‘›,ğœƒ)]ğ‘
ğ‘›=1=ğ‘â‹…ğ¸[ğ¿ğ·(ğœƒ)] 
 
Average â€“ve log likelihood. Multiply by N or not to enforce. We measure per sample 
performance to make it dataset irrelevant. KL divergence naturally appears in derivation when we try to match ğ‘(ğœƒ|ğ·) to true posterior. It tells how one diverges from other â€“ exactly what we want. Also,  KL divergence is directly 
tied to ELBO....

4. **NPTEL Summer Internship Report.pdf** (relevance: 0.150)
   Exact Bayesian Learning is possible on a limited set of models as we need to know the prior 
of the model parameters which is not feasible. Now we will talk about the scenarios where it 
is feasi ble. 3.1 Conjugate Priors  
 
3.2 When x and t are Independent â€“ SPECIAL CASE  
When x and t are independent, it means that the likelihood function does not depend on the 
input covariates x, and we can write:  
 
ğ‘(ğ‘¡|ğ‘¥,ğœƒ)=ğ‘(ğ‘¡|ğœƒ) 

16 
  
This simplification leads to a scenario where youâ€™re essentially ...

5. **NPTEL Summer Internship Report.pdf** (relevance: 0.150)
   n distribution from 
data)  
- Learning a generative model for the data  
Basically, what happens since x doesnâ€™t influence the likelihood, the learning problem becomes 
independent of the covariates. We only update the beliefs about ğœƒ based on observed data t, 
without needing to condition on x. In such a scenario the posterior ğ‘(Î¸âˆ£ğ·) is computed 
simply by updating the natural parameters of the prior with sufficient statistics from the data. ğ‘¡ğ‘›âˆ¼ğ’©(Î¼,Ïƒ2) 
Here Î¼ is unknown, Ïƒ2 is known and there...

---

